{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af90ab2-4e44-4b3e-aee7-5cca3d41e0a8",
   "metadata": {},
   "source": [
    "TextRank: https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d6d8fa-4085-4b8e-8b12-d63da6bbcb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import Parameters\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import random\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torch\n",
    "import numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516b186a-b2a7-43dc-a36f-73630a47c9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 + torch.FloatTensor([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d764fa-ce57-4878-ac32-dae5240e307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1,2,3]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6bb9390-17e6-4240-b764-6d83a78d8fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'entity_group': 'MISC', 'score': 0.9864088, 'word': 'French', 'start': 143, 'end': 149}, {'entity_group': 'LOC', 'score': 0.79081476, 'word': 'DFO Kingston Plaza', 'start': 1011, 'end': 1029}, {'entity_group': 'LOC', 'score': 0.99347866, 'word': 'Southland', 'start': 1031, 'end': 1040}]\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.desc_clean_msg_stripped_locs[df.desc_clean==\"\"\"This well-presented 3BR home is in a terrific location in highly-desirable Dingley Village and needs nothing more to do than move-in and relax. Surrounded by lush sub-tropical plants and high fences, timber decks, paved walkways and low maintenance gardens. The front master bedroom has French doors opening out to a private front garden. There is a walk-around-robe and large, sparkling ensuite that has been recently renovated. The L-shaped front lounge opens out to a private deck plus there is a truly massive family room that extends from the kitchen to the back of the house. Both large living areas have beautifully polished floors. At the rear is a covered alfresco deck which is perfect for entertaining family and friends. The kitchen is bright and fresh with a feature window-splashback and stone features plus stainless appliances including dishwasher. There are two ample bedrooms each with BIR plus a renovated family bathroom, a separate laundry and large carport. This clean and tidy home has ducted gas heating, split system AC, lots of space and little maintenance. Walk to local shops, primary school and parks or take an short trip to DFO Kingston Plaza, Southland and the beach..\"\"\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "709d55cc-7d43-4518-a42b-7f60541ae02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_potential_masks(text, locs, loc_score_thresh):\n",
    "        hot_masks = []\n",
    "        loc_masks = []\n",
    "        # for sentence in sent_tokenize(text):\n",
    "        #     for word in word_tokenize(sentence):\n",
    "        #         if word.lower() in self.hotwords.keys():\n",
    "        #             hot_masks.append({'word':word, \n",
    "        #                               'prob':self.hotwords[word.lower()], \n",
    "        #                               'tokens':self.hotword_tokens[word.lower()]})\n",
    "        # entities = self.ner(text)\n",
    "        for i,entity in enumerate(locs):\n",
    "            if entity['entity_group'] == 'LOC' and entity['score'] > loc_score_thresh and not '#' in entity['word']:\n",
    "                    loc_masks.append({'word':entity['word'], \n",
    "                                      'prob':entity['score'], \n",
    "                                      'tokens':tokenizer.encode(entity['word'])[1:-1]})\n",
    "\n",
    "        return hot_masks, loc_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3e644111-cbde-4c41-99d6-49e2a59e826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "descs = []\n",
    "locs = []\n",
    "for i,(desc,loc) in enumerate(zip(df.desc_clean, df.desc_clean_msg_stripped_locs)):\n",
    "    if isinstance(desc,str) and isinstance(loc,str) and isinstance(ast.literal_eval(loc), list):\n",
    "        descs.append(desc)\n",
    "        locs.append(ast.literal_eval(loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d1397641-42fa-4d31-b960-45d6a3fa4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch):\n",
    "    descs, locs = batch\n",
    "    return list(zip(descs, locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d60c06a5-38de-46a6-bdec-3c439000852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=list(zip(descs, locs)), batch_size=2, drop_last=True, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "aefc3f1c-4a74-499e-9af5-7611d306c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    descs, locs = batch\n",
    "    for i,(desc,loc) in enumerate(zip(descs, locs)):\n",
    "        x,y = find_potential_masks(desc, loc, 0.75)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "40814b6a-5475-4fa1-a2c7-1d1043d97f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'DFO Kingston Plaza',\n",
       "  'prob': 0.79081476,\n",
       "  'tokens': [1040, 14876, 9803, 8232]},\n",
       " {'word': 'Southland', 'prob': 0.99347866, 'tokens': [29121]}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ea4dcaa6-b736-4d1e-8a3a-e32b7dd4ed64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': ['MISC', 'LOC'],\n",
       "  'score': tensor([0.9864, 0.9911], dtype=torch.float64),\n",
       "  'word': ['French', 'Northland'],\n",
       "  'start': tensor([143,  27]),\n",
       "  'end': tensor([149,  36])},\n",
       " {'entity_group': ['LOC', 'ORG'],\n",
       "  'score': tensor([0.7908, 0.6385], dtype=torch.float64),\n",
       "  'word': ['DFO Kingston Plaza', 'Bir'],\n",
       "  'start': tensor([1011,  667]),\n",
       "  'end': tensor([1029,  670])},\n",
       " {'entity_group': ['LOC', 'LOC'],\n",
       "  'score': tensor([0.9935, 0.6741], dtype=torch.float64),\n",
       "  'word': ['Southland', 'Darebin'],\n",
       "  'start': tensor([1031, 1101]),\n",
       "  'end': tensor([1040, 1108])}]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3402d-f60b-4f12-933c-e2d9966e352d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754bd498-d07d-48af-b7ca-4b8197bdf6d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'tuple' is an illegal expression for augmented assignment (Temp/ipykernel_29528/631169534.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Colton\\AppData\\Local\\Temp/ipykernel_29528/631169534.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    x,y += torch.Tensor([1,1,1])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'tuple' is an illegal expression for augmented assignment\n"
     ]
    }
   ],
   "source": [
    "x=torch.Tensor([1,2,3])\n",
    "y=torch.Tensor([4,5,6])\n",
    "x,y += torch.Tensor([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fd597bd-79d3-4580-a30e-87944ce772b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_test_split = 0.8\n",
    "params = Parameters()\n",
    "max_input_length = params.max_input_length\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForPreTraining.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79f80e6-8895-418e-b48b-bf23d3a42b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29528/355222953.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mN_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mN_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mN_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0md_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mN_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(params.dataset_filename, low_memory=False)\n",
    "descs = [x for x in df.desc_clean.tolist() if isinstance(x,str)]\n",
    "\n",
    "N = len(descs)\n",
    "N_train = int(N * train_test_split)\n",
    "N_val = N - N_train\n",
    "d_train, d_val = torch.utils.data.dataset.random_split(descs, [N_train, N_val])\n",
    "dl_train = DataLoader(dataset=d_train, batch_size=batch_size, sampler=RandomSampler(d_train), drop_last=True, collate_fn=collate_func)\n",
    "dl_val = DataLoader(dataset=d_val, batch_size=batch_size, sampler=RandomSampler(d_val), drop_last=True, collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95a52d83-7907-4191-9d15-90ca7e94a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "descs = df.desc_clean\n",
    "locs = df.mask_locs\n",
    "dataset = []\n",
    "s = 0\n",
    "for pair in zip(descs,locs):\n",
    "    desc,loc = pair\n",
    "    if isinstance(desc,str):\n",
    "        if isinstance(loc, str):\n",
    "            if isinstance(ast.literal_eval(loc), list):\n",
    "                s += 1\n",
    "        \n",
    "# dataset = [(desc,ast.literal_eval(loc)) for desc,loc in zip(descs, locs) if isinstance(desc,str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71481c18-ea9b-472d-b7b1-66da25086b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49226"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86c2473c-8eae-44fe-a815-f9dbdbc88231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_word(tokens, tokenizer):\n",
    "    output_label = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        prob = random.random()\n",
    "        # mask token with 15% probability\n",
    "        if prob < 0.15:\n",
    "            prob /= 0.15\n",
    "\n",
    "            # 80% randomly change token to mask token\n",
    "            if prob < 0.8:\n",
    "                tokens[i] = tokenizer.mask_token\n",
    "\n",
    "            # 10% randomly change token to random token\n",
    "            elif prob < 0.9:\n",
    "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\n",
    "            # -> rest 10% randomly keep current token\n",
    "            output_label.append(tokenizer.vocab[token])\n",
    "        else:\n",
    "            # no masking token (will be ignored by loss function later)\n",
    "            output_label.append(-1)\n",
    "\n",
    "    return tokens, output_label\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length=max_input_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "def convert_example_to_features(tokens_a, tokens_b, max_seq_length, tokenizer):\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "    tokens_a, t1_label = random_word(tokens_a, tokenizer)\n",
    "    tokens_b, t2_label = random_word(tokens_b, tokenizer)\n",
    "    # concatenate lm labels and account for CLS, SEP, SEP\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0   0   0   0  0     0 0\n",
    "    #\n",
    "\n",
    "    tokens =      [tokenizer.cls_token] + tokens_a +             [tokenizer.sep_token] + tokens_b +              [tokenizer.sep_token]\n",
    "    segment_ids = [0] +      (len(tokens_a) * [0]) + [0] +       (len(tokens_b) * [1]) + [1] \n",
    "    lm_label_ids = [-1] + t1_label + [-1] + t2_label + [-1]\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    pad_amount = max_seq_length - len(input_ids)\n",
    "    input_mask = [1] * len(input_ids) + [0] * pad_amount\n",
    "    input_ids += [0] * pad_amount\n",
    "    segment_ids += [0] * pad_amount\n",
    "    lm_label_ids += [-1] * pad_amount\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, lm_label_ids\n",
    "\n",
    "def collate_func(bodies):\n",
    "    bodies_tokenized = [tokenizer.tokenize(body) for body in bodies]\n",
    "\n",
    "    max_length = max_input_length\n",
    "    half_length = int(max_length/2)\n",
    "\n",
    "    is_next_labels = []\n",
    "    mid_point = int(len(bodies)/2)\n",
    "    batch_ids, batch_mask, batch_segments, batch_lm_label_ids, batch_is_next = [], [], [], [], []\n",
    "    for i in range(mid_point):\n",
    "        is_next = 1 if random.random() < 0.5 else 0\n",
    "\n",
    "        tokens_a = bodies_tokenized[i]\n",
    "        if is_next == 0:\n",
    "            tokens_b = bodies_tokenized[i]\n",
    "        else:\n",
    "            tokens_b = bodies_tokenized[i+mid_point]\n",
    "        half_length_a = min(half_length, int(len(tokens_a) / 2))\n",
    "        half_length_b = min(half_length, int(len(tokens_b) / 2))\n",
    "        max_length_b = min(max_length, int(len(tokens_b)))\n",
    "        tokens_a = tokens_a[:half_length_a]\n",
    "        tokens_b = tokens_b[half_length_b:max_length_b]\n",
    "        input_ids, input_mask, segment_ids, lm_label_ids = convert_example_to_features(tokens_a, tokens_b, max_length, tokenizer)\n",
    "\n",
    "        batch_ids.append(input_ids)\n",
    "        batch_mask.append(input_mask)\n",
    "        batch_segments.append(segment_ids)\n",
    "        batch_lm_label_ids.append(lm_label_ids)\n",
    "        batch_is_next.append(is_next)\n",
    "\n",
    "    batch_ids = torch.LongTensor(batch_ids)\n",
    "    batch_mask = torch.LongTensor(batch_mask)\n",
    "    batch_segments = torch.LongTensor(batch_segments)\n",
    "    batch_lm_label_ids = torch.LongTensor(batch_lm_label_ids)\n",
    "    batch_is_next = torch.LongTensor(batch_is_next)\n",
    "\n",
    "    return batch_ids, batch_mask, batch_segments, batch_lm_label_ids, batch_is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f0e20a34-fe7d-4554-8057-7cabb4826522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ib, batch in enumerate(dl_train):\n",
    "    batch = tuple(t for t in batch)\n",
    "    input_ids, input_mask, segment_ids, lm_label_ids, is_next = batch\n",
    "    out = model(input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids)#, labels=lm_label_ids, next_sentence_label=is_next, return_dict=True)\n",
    "    mlm_logits = out.prediction_logits\n",
    "    is_next_logits = out.seq_relationship_logits\n",
    "    is_next_acc = is_next.eq(torch.argmax(is_next_logits, dim=1)).float().sum().item()\n",
    "    num_predicts_mlm = (~lm_label_ids.eq(-1)).sum().item()\n",
    "    mlm_acc = (lm_label_ids.view(-1).eq(torch.argmax(mlm_logits,dim=2).view(-1)).float().sum()).item()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7ac52cc4-b7e8-4d8c-89dc-da93348dce68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19353.6"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(dl_train)*16)*0.03*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0318c34c-3d7e-438c-8431-0ebffbc7ffd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228.8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*0.15*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58812bce-56e3-488a-9d8a-91c33a0b5c32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lm_label_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_56476/69133742.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mlm_label_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lm_label_ids' is not defined"
     ]
    }
   ],
   "source": [
    "(~lm_label_ids.eq(-1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7922a17d-f7a0-4c9a-8a39-4e68af05e32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -1,   -1,   -1,  ...,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,  ...,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,  ...,   -1,   -1,   -1],\n",
       "        ...,\n",
       "        [  -1, 1037,   -1,  ...,   -1,   -1,   -1],\n",
       "        [  -1,   -1, 2307,  ...,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,  ...,   -1,   -1,   -1]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d940f0b-7217-4f4e-9f2b-4847ac91f563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7b1d908-f203-4fce-bdb8-672131ec9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_next_acc = is_next.eq(torch.argmax(is_next_logits, dim=1)).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c2c2dce-6f8d-41ff-b4f4-a35aa408149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predicts = (~lm_label_ids.eq(-1)).sum().item()\n",
    "mlm_acc = (lm_label_ids.view(-1).eq(torch.argmax(mlm_logits,dim=2).view(-1)).float().sum()/num_predicts).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "914aafe0-820e-46f0-8b0b-50247bf4fa58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_next_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67f1f6bb-bc00-4164-8e2a-6c7265c16b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5214285850524902"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba55a893-fda6-4cd0-a6eb-ee47c7497eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
